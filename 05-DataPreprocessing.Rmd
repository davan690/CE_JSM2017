# Data Pre-processing

Many data analysis related books focus on models, algorithms and statistical inferences. However, in practice, raw data is usually not directly used for modeling. Data preprocessing is the process of converting raw data into clean data that is proper for modeling. A model fails for various reasons. One is that the modeler doesn't correctly preprocess data before modeling. Data preprocessing can significantly impact model results, such as imputing missing value and handling with outliers. So data preprocessing is a very critical part. 

```{r, out.width = "500px",echo=FALSE,fig.align='center'}
knitr::include_graphics("http://scientistcafe.com/images/dataclean.png")
```

In real life, depending on the stage of data cleanup, data has the following types:

1. raw data
2. Technically correct data
3. Data that is proper for the model
4. Summarized data
5. Data with fixed format

The raw data is the first-hand data that analyst pull from the database, market survey responds from your clients,  the experimental results collected by the R & D department, and so on. These data may be very rough, and R sometimes can't read them directly. The table title could be multi-line, or the format does not meet the requirements:

- Use 50% to represent the percentage rather than 0.5, so R will read it as a character;
- The missing value of the sales is represented by "-" instead of space so that R will treat the variable as character or factor type;
-  The data is in a slideshow document, or the spreadsheet is not ".csv" but ".xlsx"
- ...

Most of the time, you need to clean the data so that R can import them. Some data format requires a specific package. Technically correct data is the data, after preliminary cleaning or format conversion, that R (or another tool you use) can successfully import it.   

Assume we have loaded the data into R with reasonable column names, variable format and so on. That does not mean the data is entirely correct. There may be some observations that do not make sense, such as age is negative, the discount percentage is greater than 1, or data is missing. Depending on the situation, there may be a variety of problems with the data. It is necessary to clean the data before modeling. Moreover, different models have different requirements on the data. For example, some model may require the variables are of consistent; some may be susceptible to outliers or collinearity, some may not be able to handle categorical variables and so on. The modeler has to preprocess the data to make it proper for the specific model.

Sometimes we need to aggregate the data.  For example, add up the daily sales to get annual sales of a product at different locations.  In customer segmentation, it is common practice to build a profile for each segment. It requires calculating some statistics such as average age, average income, age standard deviation, etc. Data aggregation is also necessary for presentation, or for data visualization.

The final table results for clients need to be in a nicer format than what used in the analysis.  Usually, data analysts will take the results from data scientists and adjust the format, such as labels, cell color, highlight. It is important for a data scientist to make sure the results look consistent which makes the next step easier for data analysts. 

It is highly recommended to store each step of the data and the R code, making the whole process as repeatable as possible. The R markdown reproducible report will be extremely helpful for that. If the data changes, it is easy to rerun the process. In the remainder of this chapter, we will show the most common data preprocessing methods.

Load the R packages first:

```{r}
source("https://raw.githubusercontent.com/happyrabbit/CE_JSM2017/master/Rcode/00Require.R")
```
## Data Cleaning

After you load the data, the first thing is to check how many variables are there, the type of variables, the distributions, and data errors. Let's read and check the data:

```{r}
sim.dat <- read_csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv ")
summary(sim.dat)
```

Are there any problems? Questionnaire response Q1-Q10 seem reasonable, the minimum is 1 and maximum is 5. Recall that the questionnaire score is 1-5. The number of store transactions (store_trans) and online transactions (store_trans) make sense too. Things need to pay attention are: 

- There are some missing values. 
- There are outliers for store expenses (`store_exp`). The maximum value is 50000. Who would spend $50000 a year buying clothes? Is it an imputation error? 
- There is a negative value ( -500) in `store_exp ` which is not logical. 
- Someone is 300 years old. 

How to deal with that? Depending on the real situation, if the sample size is large enough, it will not hurt to delete those problematic samples. Here we have 1000 observations. Since marketing survey is usually expensive,  it is better to set these values as missing and imput them instead of deleting the rows. 


```{r}
# set problematic values as missings
sim.dat$age[which(sim.dat$age>100)]<-NA
sim.dat$store_exp[which(sim.dat$store_exp<0)]<-NA
# see the results
summary(subset(sim.dat,select=c("age","income")))
```

Now we will deal with the missing values in the data.

## Missing Values

Missing value imputation can be the topic for a book. This section will show some of the commonly used methods without getting too deep into the topic. Chapter 7 of the book by De Waal, Pannekoek and Scholtus [@impute1] makes a concise overview of some of the existing imputation methods. The choice of specific method depends on the actual situation. There is no method always better than the others.

One question to ask before imputation:  Is there any auxiliary information? Being aware of any auxiliary information is critical.  For example, if the system set customer who did not purchase as missing, then the real purchasing amount should be 0.  Is missing a random occurrence? If so, it may be reasonable to imput with mean or median. If not, what is the potential mechanism for the missing? For example, older people are more reluctant to disclose their ages in the questionnaire, so that the absence of age is not completely random. In this case, the missing values need to be estimated using the relationship between age and other independent variables. For example, use variables such as whether they have children, income, and other survey questions to build a model to predict age.

Also, the purpose of modeling is important for selecting imputation methods. If the goal is to interpret the parameter estimate or statistical inference, then it is important to study the missing mechanism carefully and to estimate the missing values using non-missing information as much as possible. If the goal is to predict,  people usually will not study the absence mechanism rigorously (but sometimes the mechanism is obvious). If the absence mechanism is not clear, treat it as missing at random and use mean, median, or k-nearest neighbor to imput. Since statistical inference is sensitive to missing values, researchers from survey statistics have conducted in-depth studies of various imputation schemes which focus on valid statistical inference. The problem of missing values in the prediction model is different from that in the traditional survey.  Therefore, there are not many papers on missing value imputation in the prediction model. Those who want to study further can refer to Saar-Tsechansky and Provost's comparison of different imputation methods [@missing1]and De Waal, Pannekoek and Scholtus' book [@impute1].

The following code randomly assigns some missing values to the previous data `demo`  and names the new data set `demo_missing`.


```{r,message=FALSE}
set.seed(100)
id1<-sample(1:nrow(demo),15)
id2<-sample(1:nrow(demo),10)
id3<-sample(1:nrow(demo),10)
demo_missing<-demo
demo_missing$age[id1]<-NA
demo_missing$income[id2]<-NA
demo_missing$education[id3]<-NA
summary(demo_missing)
```

### Impute missing values with median/mode

In the case of missing at random, a common method is to imput with the mean (continuous variable) or median (categorical variables). You can use `impute ()` function in `imputeMissings` package.

```{r}
# save the result as another object
demo_imp<-impute(sim.dat,method="median/mode")
# check the first 5 columns, there is no missing values in other columns
summary(demo_imp[,1:5])
```

After imputation, `demo_imp` has no missing value. This method is straightforward and widely used. The disadvantage is that it does not take into account the relationship between the variables. When there is a significant proportion of missing, it will distort the data. In this case, it is better to consider the relationship between variables and study the missing mechanism. In the example here, the missing variables are numeric. If the missing variable is a categorical/factor variable, the `impute ()` function will imput with the mode.

You can also use `preProcess ()` function, but it is only for numeric variables, and can not imput categorical variables. Since missing values here are numeric, we can use the `preProcess ()` function. The result is the same as the `impute ()` function. `PreProcess ()`  is a powerful function that can link to a variety of data preprocessing methods.  We will use the function later for other data preprocessing.

```{r}
imp<-preProcess(sim.dat,method="medianImpute")
demo_imp2<-predict(imp,sim.dat)
summary(demo_imp2[,1:5])
```

### K-nearest neighbors

K-nearest neighbor (KNN) will find the k closest samples (Euclidian distance) in the training set and imput the mean of those "neighbors". 

Use `preProcess()` to conduct KNN:

```r
imp<-preProcess(sim.dat,method="knnImpute",k=5)
# need to use predict() to get KNN result
demo_imp<-predict(imp,sim.dat)
```
```html
Error in `[.data.frame`(old, , non_missing_cols, drop = FALSE) : 
  undefined columns selected
```

Now we get an error saying “undefined columns selected”.  It is because `sim.dat` has non-numeric variables. The `preProcess()` in the first line will automatically ignore non-numeric columns so there is no error. However, there is a problem when using `predict()` to get the result. Removing those variable will solve the problem.

```{r}
# find factor columns
imp<-preProcess(sim.dat,method="knnImpute",k=5)
idx<-which(lapply(sim.dat,class)=="factor")
demo_imp<-predict(imp,sim.dat[,-idx])
summary(demo_imp[,1:3])
```

`lapply(data,class)` can return a list of column class. Here the data frame is `sim.dat` and the following code will give the list of column class:

```{r}
# only show the fist 3 elements
lapply(sim.dat,class)[1:3]
```

## Centering and Scaling

It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA and PLS. You can easily writing code yourself to conduct this transformation. Or the function `preProcess()` in package `caret` can apply this transformation to a set of predictors. 


```{r,message=FALSE}
#install packages needed
library(caret)
library(e1071)
library(gridExtra) 
library(lattice)
library(imputeMissings)
library(RANN)
library(corrplot)
library(nnet)
```


```{r,message=FALSE}
head(cars)
trans<-preProcess(cars,method=c("center","scale"))
transformed<-predict(trans,cars)
par(mfrow=c(1,2))
hist(cars$dist,main="Original",xlab="dist")
hist(transformed$dist,main="Centered and Scaled",xlab="dist")
```

Sometimes you only need to scale the variable. For example, if the model adds penalty to the parameter estimates (such as $L_2$ penalty is ridge regression and $L_1$ penalty in LASSO), the variables need to have similar scale to ensure a fair variable selection. I am heavy user of this kind of penalty-based model in my work and I used the following quantile transformation:

$$
x_{ij}^{*}=\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)}
$$

The reason to use 99% and 1% quantile instead of maximum and minimum values is to resist the impact of outliers.

It is easy to write a function to do it:

```{r,message=FALSE}
qscale<-function(dat){
  for (i in 1:ncol(dat)){
    up<-quantile(dat[,i],0.99)
    low<-quantile(dat[,i],0.01)
    diff<-up-low
    dat[,i]<-(dat[,i]-low)/diff
  }
  return(dat)
}
```

In order to illustrate, let's simulate a data set with two variables: income and age.  

```{r,message=FALSE}
set.seed(2015)
income<-sample(seq(50000,150000,by=500),95)
age<-income/2000-10
noise<-round(runif(95)*10,0)
age<-age+noise
income<-c(income,10000,15000,300000,250000,230000)
age<-c(age,30,20,25,35,95)
demo<-data.frame(income,age)
demo$education<-as.factor(sample(c("High School","Bachelor","Master","Doctor"),100,replace = T,prob =c(0.7,0.15,0.12,0.03) ))
summary(demo[,c("income","age")])
```

It is clear that income and age are not on the same scale. Now apply the function `qscale()` on the simulated data `demo`.

```{r,message=FALSE}
transformed<-qscale(demo[,c("income","age")])
summary(transformed)
```


## Resolve Skewness

[Skewness](https://en.wikipedia.org/wiki/Skewness)  is defined to be the third standardized central moment. The formula for the sample skewness statistics is:
$$ skewness=\frac{\sum(x_{i}+\bar{x})^{3}}{(n-1)v^{3/2}}$$
$$v=\frac{\sum(x_{i}=\bar{x})^{2}}{(n-1)}$$
Skewness=0 means that the destribution is symmetric, i.e. the probability of falling on either side of the distribution's  mean is equal. 

You can easily tell if a distribution is skewed by simple visualization. There are different ways may help to remove skewness such as log, square root or inverse. However it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter $\lambda$. 

$$
x^{*}=\begin{cases}
\begin{array}{c}
\frac{x^{\lambda}-1}{\lambda}\\
log(x)
\end{array} & \begin{array}{c}
if\ \lambda\neq0\\
if\ \lambda=0
\end{array}\end{cases}
$$

It is easy to see that this family includes log transformation ($\lambda=0$), square transformation ($\lambda=2$), square root ($\lambda=0.5$), inverse ($\lambda=-1$) and others in-between. We can still use function `preProcess()` in package `caret` to apply this transformation by chaning the `method` argument. 

```{r,message=FALSE}
(trans<-preProcess(cars,method=c("BoxCox")))
```

The output shows the sample size (50), number of variables (2) and the $\lambda$ estimates for each variable. After calling the `preProcess()` function, the `predict()` method applies the results to a data frame. 


```{r,message=FALSE}
transformed<-predict(trans,cars)
par(mfrow=c(1,2))
hist(cars$dist,main="Original",xlab="dist")
hist(transformed$dist,main="After BoxCox Transformation",xlab="dist")
```


An alternative is to use function `BoxCoxTrans()` in package `caret`.

```{r,message=FALSE}
(trans<-BoxCoxTrans(cars$dist))
transformed<-predict(trans,cars$dist)
skewness(transformed)
```

The estimated $\lambda$ is the same 0.5. Original skewness is 0.759 and after transformation, the skewness is  -0.01902765 which is close to 0. You can use function `skewness()` in package `e1071` to get the skewness statistics.


## Resolve Outliers

Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. You can refer to ["Detection of Outliers"](http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm) for more information. Some models are resistant to outliers (such as tree-based model and support vector machine). If a model is sensitive to outliers (such as linear regression and logistic regression), we can use _spatial sign_  transformation to minimize the problem. It projects the original sample points to the surface of a sphere by:

$$x_{ij}^{*}=\frac{x_{ij}}{\sqrt{\sum_{j=1}^{p}x_{ij}^{2}}}$$

As noted in the book "[Applied Predictive Modeling](http://appliedpredictivemodeling.com/)", 

> Since the denominator is intended to measure the squared distance to the center of the predictor's distribution, it is important to center and scale the predictor data prior to using this transformation. Note that, unlike centering or scaling, this manipulation of the predictors _transforms them as a group_. 


We can use `spatialSign()` function in `caret` to conduct spatial sign on  `demo`:


```{r,message=FALSE}
trans<-preProcess(demo[,c("income","age")],method=c("center","scale"))
transformed<-predict(trans,demo[,c("income","age")])
transformed2 <- spatialSign(transformed)
transformed2 <- as.data.frame(transformed2)
p1<-xyplot(income ~ age,
       data = transformed,
       main="Original")
p2<-xyplot(income ~ age,
       data = transformed2,
       main="After Spatial Sign")
grid.arrange(p1,p2, ncol=2)
```



## Collinearity

It is probably a technical term that many un-technical people also know. There is an excellent function in `corrplot` package with the same name `corrplot()` that can visualize correlation structure of a set of predictors. The function has option to reorder the variables in a way that reveals clusters of highly correlated ones. We add some columns to `demo` that are correlated.

```{r}
adddemo<-demo[,-3]
adddemo$added1<-sqrt(demo$age)+10
adddemo$added2<-log(demo$income)+demo$age
adddemo$added2<-log(demo$age)
adddemo$added4<-demo$income/1000+5*demo$age
adddemo$added5<-sin(demo$age)
```
The following command will produce visualization for the correlation matrix of `adddemo`.

```{r}
corrplot(cor(adddemo),order="hclust")
```
The size and color of the points are associated with the strength of corresponding correlation. Section 3.5 of "[Applied Predictive Modeling](http://appliedpredictivemodeling.com/)" presents a heuristic algorithm to remove minium number of predicitors to ensure all pairwise corelations are below a certain threshold:

> 1.  Calculate the correlation matrix of the predictors. 
1. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B).
1. Determine the average correlation between A and the other variables. Do the same for predictor B.
1. If A has a larger average correlation, remove it; otherwise, remove predictor B.
1. Repeat Step 2-4 until no absolute correlations are above the threshold.

The `findCorrelation()` function in package `caret` will apaply the above algorithm. 

```{r}
(highCorr<-findCorrelation(cor(adddemo),cutoff=.75))
# remove columns with high correlations
filter_demo<-adddemo[,-highCorr]
# correlation matrix for filtered data
corrplot(cor(filter_demo),order="hclust")
```

## Sparse Variables

Other than the highly related predictors, predictors with degenerate distributions need to be removed as well. Removing those variables can significant improve some models' performance and/or stability (such as linear regression and logistic regression but tree based model is impervious to this type of predictors). One extreme example is a variable with single value which is called zero-variance variable. 

Similarly those variables with very low frequency of unique values are **_near-zero variance predictors_**. How to detect those variables? There are two rules:
-  The fraction of unique values over the sample size 
-	The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value. The `caret` package funciton `nearZeroVar()` can filter near-zero variance predictors. 

```{r}
#add two variables with low variance 
zero_demo<-demo
zero_demo$zero1<-rep(0,nrow(demo))
zero_demo$zero2<-c(1,rep(0,nrow(demo)-1))
# zero1 only has one unique value
# zero2 is a vector with the first element 1 and the rest are 0s
summary(zero_demo)
# the function will return a vector of integers indicating which columns to remove
nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10)
```

Note the two arguments in the function `freqCut =` and `uniqueCut = `. They are corresponding to the previous two rules.

- `freqCut`: the cutoff for the ratio of the most common value to the second most common value

- `uniqueCut`:the cutoff for the percentage of distinct values out of the number of total samples

## Re-encode Dummy Variables

Sometimes we need to recode categories to smaller bits of information named **"dummy variables"**. Take the variable "education" in demo for example. It has four categories: "High School","Bachelor","Master" and "Doctor". If we recode it to be dummy variables, each category get its own dummy variable that is 0/1 indicator for that category. 

For a single categorical variable, we can use function `class.ind()` in package `nnet`:

```{r}
dumVar<-class.ind(demo$education)
head(dumVar)
```

If we want to determine encodeings for more than one variables, we can use `dummyVars()` in `caret`. 

```{r}
dumMod<-dummyVars(~income+education,
                  data=demo,
                  # Remove the variable name from the column name
                  levelsOnly=T)
predict(dumMod,head(demo))
```

To add some more complexity, we could assume _joint_  effect of income and education. In this case, this will add 4 more columns to the resulted data frame:

```{r}
dumMod<-dummyVars(~income+education+income:education,
                  data=demo,
                  levelsOnly=T)
predict(dumMod,head(demo))
```


