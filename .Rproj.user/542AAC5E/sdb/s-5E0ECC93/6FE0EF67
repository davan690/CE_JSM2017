{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Data Wrangling\"\nauthor: \"Ming Li, Amazon </br> [Hui Lin](http://scientistcafe.com), DuPont\"\ndate: \"July 30, 2017 @ JSM2017\"\noutput: \n  slidy_presentation: \n    footer: \"http://scientistcafe.com\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n## Issue driven\n\n> - If you don't know where to go ...\n\n> - <img src=\"images/watch.gif\" alt=\"HTML5 Icon\" style=\"width:300px;height:200px;\">\n\n> - If you know where to go ...\n\n> - <img src=\"images/fun.gif\" alt=\"HTML5 Icon\" style=\"width:300px;height:200px;\">\n\n\n## Outline\n\n- Read and write data\n    - `readr`\n    - `data.table` — enhanced `data.frame`\n- Summarize data\n    - `apply()`, `lapply()` and `sapply()` in base R\n    - `ddply()` in `plyr` package\n    - `dplyr` package\n- Tidy and Reshape Data\n    - `reshape2` package\n    - `tidyr` package\n\n## Read and write data\n\n1. It is 10x faster.\n1. It doesn't change the column names.\n\n    ```r\n    library(readr)\n    read_csv(\"2015,2016,2017\n    1,2,3\n    4,5,6\")\n    ```\n    \n1. It does not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column. \n1. Killing character : **progress bar**. \n\n## Major functions\n\n- `read_csv()`: reads comma delimited files\n- `read_csv2()`: reads semicolon separated files (common in countries where  `,`  is used as the decimal place)\n- `read_tsv()`: reads tab delimited files\n- `read_delim()`: reads in files with any delimiter\n- `read_fwf()`: reads fixed width files. You can specify fields either by their widths with `fwf_widths()`  or their position with  `fwf_positions()`  \n- `read_table()`: reads a common variation of fixed width files where columns are separated by white space \n- `read_log()`: reads Apache style log files\n\n## `read_csv()`\n\n```r\nlibrary(readr)\nsim.dat <- read_csv(\"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv \")\nhead(sim.dat)\n```\n\n- `tibble`\n    - It never changes an input’s type (i.e., no more `stringsAsFactors = FALSE`!)\n    - It never adjusts the names of variables\n    - It has a refined print method that shows only the first 10 rows, and all the columns that fit on screen. You can also control the default print behavior by setting options.\n\nRefer to http://r4ds.had.co.nz/tibbles.html for more information about ‘tibble’.\n\n## Baby example 1\n\n```r\ndat=read_csv(\"2015,2016,2017\n100,200,300\ncanola,soybean,corn\")\nprint(dat)\n```\n\n## Baby example 2\n\nYou can also add comments on the top and tell R to skip those lines:\n\n```r\ndat=read_csv(\"# I will never let you know that\n          # my favorite food is carrot\n          Date,Food,Mood\n          Monday,carrot,happy\n          Tuesday,carrot,happy\n          Wednesday,carrot,happy\n          Thursday,carrot,happy\n          Friday,carrot,happy\n          Saturday,carrot,extremely happy\n          Sunday,carrot,extremely happy\", skip = 2)\nprint(dat)\n```\n\n## Baby example 3\n\n- If you don't have column names, set `col_names = FALSE` then R will assign names \"`X1`\",\"`X2`\"... to the columns:\n\n```r\ndat=read_csv(\"Saturday,carrot,extremely happy\n          Sunday,carrot,extremely happy\", col_names=FALSE)\nprint(dat)\n```\n\n- You can also pass `col_names`  a character vector which will be used as the column names. Try to replace `col_names=FALSE` with `col_names=c(\"Date\",\"Food\",\"Mood\")` and see what happen.\n\n## `read_csv2()`\n\n- Read semicolon separated files:\n\n```r\ndat=read_csv2(\"Saturday; carrot; extremely happy \\n Sunday; carrot; extremely happy\", col_names=FALSE)\nprint(dat)\n```\n\n## `read_tsv()`\n\n- Read tab delimited files：\n\n```r\ndat=read_tsv(\"every\\tman\\tis\\ta\\tpoet\\twhen\\the\\tis\\tin\\tlove\\n\", col_names = FALSE)\nprint(dat)\n```\n\n## `read_delim()`\n\n- Assign separating character\n\n```r\ndat=read_delim(\"THE|UNBEARABLE|RANDOMNESS|OF|LIFE\\n\", delim = \"|\", col_names = FALSE)\nprint(dat)\n```\n\n## Substitute for missing value\n\n```r\ndat=read_csv(\"Q1,Q2,Q3\n               5, 4,99\",na=\"99\")\nprint(dat)\n```\n\n## Write data \n\n- use `write_csv()` and `write_tsv()`\n- Encode strings in UTF-8\n- Save dates and date-times in ISO8601 format so they are easily parsed elsewhere\n\n\n```r\nwrite_csv(sim.dat, \"sim_dat.csv\")\n```\n\n## Other data types\n\n- `Haven`: SPSS, Stata and SAS data\n- `Readxl` and `xlsx`: excel data(.xls and .xlsx)\n- `DBI`: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL\n\nUseful materials:\n\n- For getting data from internet, you can refere to the book “XML and Web Technologies for Data Sciences with R”. \n- [R data import/export manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html#Acknowledgements)\n- `rio` package：https://github.com/leeper/rio\n\n## `data.table`--- enhanced `data.frame`\n\n- What is `data.table`? \n    - It is an R package that provides an enhanced version of `data.frame`. \n    \n- Advantages:\n    - Offers fast import, subset, grouping, update, and joins for large data files\n    - It is easy to turn data frame to data table\n    - Can behave just like a data frame\n\n```r\n# If you haven't install it, use the code to instal\n# install.packages(\"data.table\")\n# load packagw\nlibrary(data.table)\n# covert the existing data frame to data table\ndt <- data.table(sim.dat)\nclass(dt)\n```\n\n## Manipulating data table\n\n- Calculate mean for counts of online transactions:\n\n```r\ndt[, mean(online_trans)]\n```\n\n- You can't do the same thing using data frame:\n\n```r\nsim.dat[,mean(online_trans)]\n```\n\n## By group\n\n- Calculate mean by group:\n\n```r\ndt[ , mean(online_trans), by = gender]\n```\n\n- You can group by more than one variables:\n\n```r\ndt[ , mean(online_trans), by = .(gender, house)]\n```\n\n- Assign column names for aggregated variables:\n\n```r\ndt[ , .(avg = mean(online_trans)), by = .(gender, house)]\n```\n\n##  General setting of `data.table`\n\n- Different from data frame, there are three arguments for data table:\n\n<center>\n![](http://scientistcafe.com/book/Figure/datable1.png)\n</center>\n\n- analogous to SQL\n\n<center>\n![](http://scientistcafe.com/book/Figure/rSQL.png)\n</center>\n\n\n## Equivalent codes\n\n```r\n# Rcode 1\ndt[ , mean(online_trans), by = gender]\n```\n\n```sql\n# SQL 1\nSELECT  gender, avg(online_trans) FROM sim.dat GROUP BY gender\n```\n\n```r\n# Rcode 2\ndt[ , .(avg = mean(online_trans)), by = .(gender, house)]\n```\n\n\n```sql \n# SQL 2\nSELECT gender, house, avg(online_trans) AS avg FROM sim.dat GROUP BY gender, house\n```\n\n```r\n# Rcode 3\ndt[ age < 40, .(avg = mean(online_trans)), by = .(gender, house)]\n```\n\n\n```sql\n# SQL 3\nSELECT gender, house, avg(online_trans) AS avg FROM sim.dat WHERE age < 40 GROUP BY gender, house\n```\n\n## Subset\n\n- select row\n\n```r\n# select rows with age<20 and income > 80000\ndt[age < 20 & income > 80000]\n# select the first two rows\ndt[1:2]\n```\n- select column\n\n```r\n# select column “age” but return it as a vector\n# the argument for row is empty so the result will return all observations\nans <- dt[, age]\nhead(ans)\n# To return `data.table` object, put column names in `list()`\n# Select age and online_exp columns and return as a data.table instead\nans <- dt[, list(age, online_exp)]\nhead(ans)\n# you can also put column names in `.()`\nans <- dt[, .(age, online_exp)]\n# head(ans)\n# To select all columns from “age” to “income”\nans <- dt[, age:income, with = FALSE]\nhead(ans,2)\n# Delete columns using `-` or `!`\n# delete columns from  age to online_exp\nans <- dt[, -(age:online_exp), with = FALSE]\nans <- dt[, !(age:online_exp), with = FALSE]\n```\n\n## tabulation\n\n- In data table. `.N` means to count。\n\n```r\n# row count\ndt[, .N] \n```\n\n- Count by groups:\n\n```r\n# counts by gender\ndt[, .N, by= gender]  \n# for those younger than 30, count by gender\n dt[age < 30, .(count=.N), by= gender] \n```\n\n- Order table:\n\n```r\n# get records with the highest 5 online expense:\nhead(dt[order(-online_exp)],5) \n# Since data table keep some characters of data frame, they share some operations\ndt[order(-online_exp)][1:5]\n# order the table by more than one variable\ndt[order(gender, -online_exp)][1:5]\n```\n\n## Use `fread()` to import dat\n\n```r\nsystem.time(topic<-read.csv(\"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv\"))\n```\n\n```html\n  user  system elapsed \n  4.313   0.027   4.340\n```\n\n```r\nsystem.time(topic<-readr::read_csv(\"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv\"))\n```\n\n```html\n   user  system elapsed \n  0.267   0.008   0.274 \n```\n\n```r\nsystem.time(topic<-data.table::fread(\"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv\"))\n```\n\n```html\n   user  system elapsed \n  0.217   0.005   0.221 \n```\n\n## `apply()`, `lapply()` and `sapply()` in base R\n\n- Similarity: apply functions over parts of data\n- Difference:\n    - the type of object they apply to\n    - the type of result they will return\n    \n## When do we use `apply()`?\n\n- apply a function to margins of an array or matrix\n- returns a vector or array or list of values\n\n```r\n## simulate a matrix\nx <- cbind(x1 =1:8, x2 = c(4:1, 2:5))\ndimnames(x)[[1]] <- letters[1:8]\napply(x, 2, mean)\ncol.sums <- apply(x, 2, sum)\nrow.sums <- apply(x, 1, sum)\n```\n\n- apply other functions:\n\n```r\nma <- matrix(c(1:4, 1, 6:8), nrow = 2)\nma\napply(ma, 1, table)  #--> a list of length 2\napply(ma, 1, stats::quantile) # 5 x n matrix with rownames\n```\n\n## Tricky example\n\n- Results can have different lengths for each call. \n- This is a trickier example. What will you get? \n\n```r\n## Example with different lengths for each call\nz <- array(1:24, dim = 2:4)\nzseq <- apply(z, 1:2, function(x) seq_len(max(x)))\nzseq         ## a 2 x 3 matrix\ntypeof(zseq) ## list\ndim(zseq) ## 2 3\nzseq[1,]\napply(z, 3, function(x) seq_len(max(x)))\n```\n\n## `lapply()` and `sapply()`\n\n- `lapply()` applies a function over a list, data.frame or vector and returns a list of the same length.\n- `sapply()` is a user-friendly version and wrapper of `lapply()`. By default it returns a vector, matrix or if  `simplify = \"array\"`, an array if appropriate. `apply(x, f, simplify = FALSE, USE.NAMES = FALSE)` is the same as `lapply(x, f)`. If `simplify=TRUE`, then it will return a `data.frame` instead of `list`. \n\n## Example using data with context\n\n- Get the mean and standard deviation of all numerical variables in the data set.\n\n```r\n# Read data\nsim.dat<-read.csv(\"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv\")\n# Get numerical variables\nsdat<-sim.dat[,!lapply(sim.dat,class)==\"factor\"]\n## Try the following code with apply() function\n## apply(sim.dat,class)\n## Why is there the error?\n```\n\n- use `apply()` to get mean and standard deviation for each column\n\n```r\napply(sdat, MARGIN=2,function(x) mean(na.omit(x)))\napply(sdat, MARGIN=2,function(x) sd(na.omit(x)))\n```\n\n- Run the following code and compare the results:\n\n```r\nlapply(sdat, function(x) sd(na.omit(x)))\nsapply(sdat, function(x) sd(na.omit(x)))\nsapply(sdat, function(x) sd(na.omit(x)), simplify = FALSE)\n```\n\n## `dplyr` package\n\n- Next iteration of `plyr` package\n- Flexible grammar of data manipulation focusing on tools for working with data frames (hence the `d` in the name)\n- It identifies the most important data manipulations and make they easy to use from R\n- It performs faster for in-memory data by writing key pieces in C++ using `Rcpp`\n\nI will illustrate the following functions in order: \n\n1. Display\n1. Subset\n1. Summarize\n1. Create new variable\n1. Merge\n\n## Display\n\n- `tbl_df()`: Convert the data to `tibble` \n\n```r\nlibrary(dplyr)\ntbl_df(sim.dat)\n```\n\n- `glimpse()`: This is like a transposed version of `tbl_df()`\n\n```r\nglimpse(sim.dat)\n```\n\n## Subset\n\n- Get rows with `income` more than 300000:\n\n```r\nfilter(sim.dat, income >300000) %>%\n  tbl_df()\n```\n\n- Here we meet a new operator `%>%`: \"Pipe operator\" \n\n## Pipe operator: `%>%`\n\n- It pipes a value forward into an expression or function call\n- What you get in the left operation will be the first argument or the only argument in the right operation.\n\n```r\nx %>% f(y) = f(x, y)\ny %>% f(x, ., z) = f(x, y, z )\n```\n\nFor example: `\"Hello World\" %>% substring(7, 11) %>% grepl(\"Wo\", .)`\n\n## Pipe operator: `%>%`\n\nLook at the following code. Can you tell me what it does?\n\n```r\nave_exp <- filter( \n  summarise(\n    group_by( \n      filter(\n        sim.dat, \n        !is.na(income)\n      ), \n      segment\n    ), \n    ave_online_exp = mean(online_exp), \n    n = n()\n  ), \n  n > 200\n) \n```\nNow look at the identical code using \"`%>%`\":\n\n```r\navg_exp <- sim.dat %>% \n filter(!is.na(income)) %>% \n group_by(segment) %>% \n summarise( \n   ave_online_exp = mean(online_exp), \n   n = n() ) %>% \n  filter(n > 200)\n```\n\n## Pipe operator: `%>%`\n\n```r\navg_exp <- sim.dat %>% \n filter(!is.na(income)) %>% \n group_by(segment) %>% \n summarise( \n   ave_online_exp = mean(online_exp), \n   n = n() ) %>% \n  filter(n > 200)\n```\n\nIsn't it much more straight forward now? Let's read it:\n\n1.\tDelete observations from `sim.dat` with missing income values \n2.\tGroup the data from step 1 by variable `segment`\n3.\tCalculate mean of online expense for each segment and save the result as a new variable named `ave_online_exp`\n4.\tCalculate the size of each segment and saved it as a new variable named `n`\n5.\tGet segments with size larger than 200\n\n## Subset - select rows\n \n- `distinct()`: a generalization of `unique()` from vector to data frame\n\n```r\ndplyr::distinct(sim.dat)\n```\n\n- `sample_frac()`: randomly select some rows with specified percentage. \n- `sample_n()`:randomly select rows with specified number.\n\n```r\ndplyr::sample_frac(sim.dat, 0.5, replace = TRUE) \ndplyr::sample_n(sim.dat, 10, replace = TRUE) \n```\n\n- `slice()` will select rows by position:\n\n```r\n# It is equivalent to `sim.dat[10:15,]`\ndplyr::slice(sim.dat, 10:15) \n```\n\n- `top_n()`  select the order top n entries:\n\n```r\ndplyr::top_n(sim.dat,2,income)\n```\n\n## Subset - select columns\n\n```r\n# select by column name\ndplyr::select(sim.dat,income,age,store_exp)\n\n# select columns whose name contains a character string\ndplyr::select(sim.dat, contains(\"_\"))\n\n# select columns whose name ends with a character string\n# similar there is \"starts_with\"\ndplyr::select(sim.dat, ends_with(\"e\"))\n\n# select columns Q1,Q2,Q3,Q4 and Q5\nselect(sim.dat, num_range(\"Q\", 1:5)) \n\n# select columns whose names are in a group of names\ndplyr::select(sim.dat, one_of(c(\"age\", \"income\")))\n\n# select columns between age and online_exp\ndplyr::select(sim.dat, age:online_exp)\n\n# select all columns except for age\ndplyr::select(sim.dat, -age)\n```\n\n## Summarize\n\n```r\ndplyr::summarise(sim.dat, avg_online = mean(online_trans)) \n# apply function anyNA() to each column\n# you can also assign a function vector such as: c(\"anyNA\",\"is.factor\")\ndplyr::summarise_each(sim.dat, funs_(c(\"anyNA\")))\n```\n\n- `group_by()` \n\n```r\nsim.dat %>% group_by(segment) %>% summarise_each(funs_(c(\"anyNA\")))\n```\n\n## Create new variable\n\n- `mutate()`: compute and append one or more new columns:\n\n```r\ndplyr::mutate(sim.dat, total_exp = store_exp + online_exp)\n```\n\n- It will apply **window function** to the columns and return a column with the same length\n\n```r\n# min_rank=rank(ties.method = \"min\")\n# mutate_each() means apply function to each column\ndplyr::mutate_each(sim.dat, funs(min_rank)) \n```\n\n-  `transmute()`: delete the original columns and only keep the new ones\n\n```r\ndplyr::transmute(sim.dat, total_exp = store_exp + online_exp) \n```\n\n## Merge\n\n```r\n(x<-data.frame(cbind(ID=c(\"A\",\"B\",\"C\"),x1=c(1,2,3))))\n(y<-data.frame(cbind(ID=c(\"B\",\"C\",\"D\"),y1=c(T,T,F))))\n```\n\n```r\n# join to the left\n# keep all rows in x\nleft_join(x,y,by=\"ID\")\n# get rows matched in both data sets\ninner_join(x,y,by=\"ID\")\n# get rows in either data set\nfull_join(x,y,by=\"ID\")\n# filter out rows in x that can be matched in y \n# it doesn't bring in any values from y \nsemi_join(x,y,by=\"ID\")\n# the opposite of  semi_join()\n# it gets rows in x that cannot be matched in y\n# it doesn't bring in any values from y\nanti_join(x,y,by=\"ID\")\n```\n\n## Tidy and Reshape Data \n\n\n-  \"Tidy data\" represent the information from a dataset as data frames where each row is an observation and each column contains the values of a variable\n- convert data between the \"wide\" and the \"long\" format\n- two commonly used packages for this kind of manipulations: `tidyr` and `reshape2`\n\n## `reshape2` package\n\n- reboot of previous package `reshape`\n- main functions:\n    1. `melt()` to convert an object into a molten data frame, i.e. from wide to long\n    1. `dcast()` to cast a molten data frame into the shape you want, i.e. from long to wide\n\n```r\n# Take a baby subset of our exemplary clothes consumers data to illustrate:\n(sdat<-sim.dat[1:5,1:6])\n```\n\n## `reshape2` example\n\n- have a variable indicating the purchasing channel (i.e. online or in-store) and another column with the corresponding expense amount\n\n```r\nlibrary(reshape2)\n(mdat <- melt(sdat, measure.vars=c(\"store_exp\",\"online_exp\"),\n              variable.name = \"Channel\",\n              value.name = \"Expense\"))\n```\n\n\n- You can run a regression to study the effect of purchasing channel: \n\n```r\n# Here we use all observations from sim.dat\nmdat<-melt(sim.dat[,1:6], measure.vars=c(\"store_exp\",\"online_exp\"),\n            variable.name = \"Channel\",\n              value.name = \"Expense\")\nfit<-lm(Expense~gender+house+income+Channel+age,data=mdat)\nsummary(fit)\n```\n\n## `reshape2` example\n\n- compare the online and in store expense between male and female based on the house ownership\n\n```r\ndcast(mdat, house + gender ~ Channel, sum)\n```\n\n- left side : variables that you want to group by\n- right side: variable you want to spread as columns\n\n## `tidyr` package\n\n- Get a baby set to illustrate:\n\n```r\nlibrary(dplyr)\nlibrary(tidyr)\n# practice functions we learnt before\nsdat<-sim.dat[1:5,]%>%\n  dplyr::select(age,gender,store_exp,store_trans)\nsdat %>% tbl_df()\n```\n\n## `gather()`\n\n- Analogous to `melt()` in `reshape2`\n\n```r\nlibrary(tidyr)\nmsdat<-tidyr::gather(sdat,\"variable\",\"value\",store_exp,store_trans)\nmsdat %>% tbl_df()\n```\n\n-  if we use the pipe operation,\n\n```r\nsdat%>%gather(\"variable\",\"value\",store_exp,store_trans)\n```\n\n- It is identical with the following code using `melt()`:\n\n```r\nlibrary(reshape2)\nmelt(sdat, measure.vars=c(\"store_exp\",\"store_trans\"),\n            variable.name = \"variable\",\n              value.name = \"value\")\n```\n\n## `spread()`\n\n\n```r\nmsdat %>% spread(variable,value)\n```\n\n##  `separate()` and `unite()`\n\n```r\n# You can use `sep=` \n# By default, it is \"`_`\"\nsepdat<- msdat %>% \n  separate(variable,c(\"Source\",\"Type\"))\nsepdat %>% tbl_df()\n```\n\n```r\nsepdat %>% \n  unite(\"variable\",Source,Type,sep=\"_\")\n```\n\n## Some links\n\n- SQL Learning: https://www.w3schools.com/sql/\n\n",
    "created" : 1501360461903.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1490629291",
    "id" : "6FE0EF67",
    "lastKnownWriteTime" : 1501360781,
    "last_content_update" : 1501360781,
    "path" : "~/Documents/GitHub/CE_JSM2017/Slides/DataWrangling.Rmd",
    "project_path" : "Slides/DataWrangling.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}