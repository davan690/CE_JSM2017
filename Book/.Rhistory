p_needed <- c("plyr", "dplyr", "caret","e1071","gridExtra","lattice","imputeMissings",
"RANN","corrplot","nnet","car","gpairs","reshape2","psych","tidyr")
packages <- rownames(installed.packages())
p_needed[!(p_needed %in% packages)]
bookdown::render_book("index.Rmd", "bookdown::gitbook")
source("https://raw.githubusercontent.com/happyrabbit/CE_JSM2017/Rcode/00-course-setup.R")
source("https://raw.githubusercontent.com/happyrabbit/CE_JSM2017/master/Rcode/00-course-setup.R")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
devtools::install_github('yihui/xaringan')
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
source("https://raw.githubusercontent.com/happyrabbit/CE_JSM2017/master/Rcode/00-course-setup.R")
sim.dat <- read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv ")
library(DataScienceR)
data("sim1_da1")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
load("/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/avnnetTune.RData")
avnnetTune
load("/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/gbmTuneNN.RData")
trainx = dplyr::select(sim1_da1, -y)
trainy = sim1_da1$y
bookdown::render_book("index.Rmd", "bookdown::gitbook")
cat(readLines('segmentdb_ui.csv'), sep = '\n')
cat(readLines('Data/segmentdb_ui.csv'), sep = '\n')
cat(readLines('Data/segmentdb_server.csv'), sep = '\n')
bookdown::render_book("index.Rmd", "bookdown::gitbook")
library(DT)
library(dplyr)
sim.dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
# Summarise data
seg<-sim.dat%>%
filter(age < 100)%>%
group_by(segment)%>%
summarise(Age=round(mean(na.omit(age)),0),
FemalePct=round(mean(gender=="Female"),2),
HouseYes=round(mean(house=="Yes"),2),
store_exp=round(mean(na.omit(store_exp),trim=0.1),0),
online_exp=round(mean(online_exp),0),
store_trans=round(mean(store_trans),1),
online_trans=round(mean(online_trans),1))%>%
data.frame()
# show summarized data by interactive table
datatable( seg,
# no row names
rownames = FALSE,
# Assign colomn names for output table
colnames = c('Segment', 'Age', 'Female %', 'House Owner',
'Store $','Online $', 'Store #', 'Online #' ),
# Define table CSS Classes
class = "cell-border stripe",
# Define table caption
caption = 'Table 1: Segment Summary Table',
options = list(
# show the first 4 rows
pageLength = 4,
# Enable automatic column width calculation
autoWidth = TRUE)
)
View(seg)
View(seg)
datatable( seg,
# no row names
rownames = FALSE,
# Assign colomn names for output table
colnames = c('Segment', 'Age', 'Female Pct', 'House Owner',
'Store','Online', 'Store Count', 'Online Count' ),
# Define table CSS Classes
class = "cell-border stripe",
# Define table caption
caption = 'Table 1: Segment Summary Table',
options = list(
# show the first 4 rows
pageLength = 4,
# Enable automatic column width calculation
autoWidth = TRUE)
)
datatable( seg,
# no row names
rownames = FALSE,
# Assign colomn names for output table
colnames = c('Segment', 'Age', 'Female Pct', 'House Owner',
'Store','Online', 'Store Count', 'Online Count' ),
# Define table CSS Classes
class = "cell-border stripe",
# Define table caption
caption = 'Table 1: Segment Summary Table',
options = list(
# show the first 4 rows
pageLength = 4,
# Enable automatic column width calculation
autoWidth = TRUE)
)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
load("https://github.com/happyrabbit/DataScientistR/blob/master/Data/nnetTune.RData")
load("https://github.com/happyrabbit/DataScientistR/blob/master/Data/nnetTune.RData")
load("/Data/gbmTuneNN.RData")
load("Data/gbmTuneNN.RData")
load("Data/gbmTuneNN.RData")
load("Data/nnetTune.RData")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
source("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/R_Code/multiplot.r")
library(grid)
library(lattice)
library(ggplot2)
source("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/R_Code/multiplot.r")
# randomly simulate some non-linear samples
x=seq(1,10,0.01)*pi
e=rnorm(length(x),mean=0,sd=0.2)
fx<-sin(x)+e+sqrt(x)
dat=data.frame(x,fx)
# plot fitting result
ggplot(dat,aes(x,fx))+
geom_point() +
geom_smooth(method = "lm", se = FALSE)
p_needed <- c("plyr", "dplyr", "caret","e1071","gridExtra","lattice","imputeMissings",
"RANN","corrplot","nnet","car","gpairs","reshape2","psych","tidyr",
"ggplot2","sparklyr","readr","mvtnorm","MASS","data.table","magrittr",
"shiny","metricsgraphics","DT","leaflet","dygraphs","xts","lubridate",
"highcharter","rbokeh","networkD3","threejs")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
p_to_install
install.packages("revealjs")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
p_needed <- c("plyr", "dplyr", "caret","e1071","gridExtra","lattice","imputeMissings",
"RANN","corrplot","nnet","car","gpairs","reshape2","psych","tidyr",
"ggplot2","sparklyr","readr","mvtnorm","MASS","data.table","magrittr",
"shiny","metricsgraphics","DT","leaflet","dygraphs","xts","lubridate",
"highcharter","rbokeh","networkD3","threejs","knitr")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
p_to_install
indir<-"/Users/happyrabbit/Documents/NLP/ch10"
wikiFiles = dir(paste0(indir,"wiki_annotations/"), full.names=TRUE)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
lemmas = c()
for (f in wikiFiles) {
anno = readRDS(f)
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
lemmas = append(lemmas, theseLemma)
}
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
library(mallet)
library(coreNLP)
library(mallet)
library(coreNLP)
indir<-"/Users/happyrabbit/Documents/NLP/ch10"
wikiFiles = dir(paste0(indir,"wiki_annotations/"), full.names=TRUE)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
lemmas = c()
for (f in wikiFiles) {
anno = readRDS(f)
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
lemmas = append(lemmas, theseLemma)
}
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
library(rjava)
install.packages("rjava")
install.packages("rjava")
install.packages("rJava")
install.packages("rJava")
library(rJava)
indir<-"/Users/happyrabbit/Documents/NLP/ch10"
wikiFiles = dir(paste0(indir,"wiki_annotations/"), full.names=TRUE)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
lemmas = c()
for (f in wikiFiles) {
anno = readRDS(f)
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
lemmas = append(lemmas, theseLemma)
}
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
library(rJava)
library(mallet)
library(coreNLP)
indir<-"/Users/happyrabbit/Documents/NLP/ch10"
wikiFiles = dir(paste0(indir,"wiki_annotations/"), full.names=TRUE)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
lemmas = c()
for (f in wikiFiles) {
anno = readRDS(f)
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
lemmas = append(lemmas, theseLemma)
}
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
library(mallet)
library(coreNLP)
library(rJava)
indir<-"/Users/happyrabbit/Documents/NLP/ch10"
wikiFiles = dir(paste0(indir,"wiki_annotations/"), full.names=TRUE)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
lemmas = c()
for (f in wikiFiles) {
anno = readRDS(f)
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
lemmas = append(lemmas, theseLemma)
}
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
wikiNames
indir<-"/Users/happyrabbit/Documents/NLP/ch10/"
wikiFiles = dir(paste0(indir,"wiki_annotations"), full.names=TRUE)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
wikiNames
lemmas = c()
for (f in wikiFiles) {
anno = readRDS(f)
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
lemmas = append(lemmas, theseLemma)
}
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
j = 1
f = 1
lemmas = names(sort(table(lemmas),decreasing=TRUE)[1:50])
lemmas
tf = matrix(0,nrow=length(wikiFiles),ncol=length(lemmas))
colnames(tf) = lemmas
rownames(tf) = substr(wikiNames,nchar(wikiNames)-10,nchar(wikiNames))
j =1
anno = readRDS(wikiFiles[j])
token = getToken(anno)
coreNLP::getDependency(anno)
dep= getDependency(anno)
View(token)
wikiNames = gsub("\\.Rds", "", basename(wikiFiles))
wikiNames
View(dep)
tf = matrix(0,nrow=length(wikiFiles),ncol=length(lemmas))
colnames(tf) = lemmas
rownames(tf) = substr(wikiNames,nchar(wikiNames)-10,nchar(wikiNames))
j =1
for (j in 1:length(wikiFiles)) {
anno = readRDS(wikiFiles[j])
token = getToken(anno)
dep= getDependency(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
theseLemma = theseLemma[theseLemma %in% lemmas]
tab = table(theseLemma)
index = match(lemmas,names(tab))
tf[j,!is.na(index)] = tab[index[!is.na(index)]]
}
tf[1:6,1:6]
lemmas[apply(tf,1,which.max)][1:10]
df = apply(tf!=0,2,sum)
df[1:20]
df = matrix(rep(df,length(wikiFiles)),ncol=length(lemmas),
byrow=TRUE) / length(wikiFiles)
impScore = round(tf * log(1/df),3)
impScore[1:6,1:8]
sort(impScore["Machiavelli",],decreasing=TRUE)[1:6]
sort(impScore["Jean_Piaget",],decreasing=TRUE)[1:6]
sort(impScore["oam_Chomsky",],decreasing=TRUE)[1:6]
sort(impScore["ohn_Paul_II",],decreasing=TRUE)[1:6]
#########################
# Topic Models
dateSet = rep(0L,length(wikiFiles))
for (j in 1:length(wikiFiles)) {
anno = readRDS(wikiFiles[j])
tx = getToken(anno)$Timex
tx = substr(tx[!is.na(tx)],1,4)
tx = as.numeric(tx)
tx = tx[!is.na(tx)]
dateSet[j] = tx[1]
}
wikiFiles = wikiFiles[order(dateSet)]
wikiNames = wikiNames[order(dateSet)]
shortNames = sapply(strsplit(wikiNames, "_"), function(v) rev(v)[1])
shortNames[115] = "Godel"
shortNames[135] = "John Paul II"
shortNames[156] = "Newman"
shortNames[174] = "Mao"
bagOfWords = rep("",length(wikiFiles))
for (j in 1:length(wikiFiles)) {
anno = readRDS(wikiFiles[j])
token = getToken(anno)
theseLemma = token$lemma[token$POS %in% c("NNS","NN")]
bagOfWords[j] = paste(theseLemma,collapse=" ")
}
tf = tempfile()
writeLines(c(letters,LETTERS),tf)
instance = mallet.import(wikiNames, bagOfWords, tf)
tm = MalletLDA(9)
tm$loadDocuments(instance)
tm$setAlphaOptimization(30,50)
tm$train(200)
tm$maximize(10)
topics = mallet.doc.topics(tm, smoothed=TRUE, normalized=TRUE)
words = mallet.topic.words(tm, smoothed=TRUE, normalized=TRUE)
vocab = tm$getVocabulary()
#tmSaved = list(topics=topics,words=words,vocab=vocab)
#saveRDS(tmSaved, "~/Desktop/tm.Rds")
tm = readRDS(paste0(indir,"tm.Rds"))
topics = tm$topics
words = tm$words
vocab = tm$vocab
dim(topics)
dim(words)
length(vocab)
t(apply(words,1,function(v) vocab[order(v,decreasing=TRUE)[1:5]]))
topicNames = c("politics","biography","social-science","existentialism",
"philosophy","logic","poetry","culture","language")
index = order(apply(words,2,max),decreasing=TRUE)[1:50]
set = unique(as.character(apply(words,1,function(v)
vocab[order(v,decreasing=TRUE)[1:5]])))
index = match(set,vocab)
mat = round(t(words[,index]),3)
mat = mat / max(mat)
#pdf(paste0(OUTDIR, "topicWordDistribution.pdf"), 6, 8.5)
par(mar=c(0,0,0,0))
plot(0,0,col="white",ylim=c(-1,nrow(mat)),xlim=c(-2,ncol(mat)))
for(i in 1:nrow(mat)) lines(x=c(1,ncol(mat)),y=c(i,i), col=grey(0.5,0.5))
for(i in 1:ncol(mat)) lines(x=c(i,i),y=c(1,nrow(mat)), col=grey(0.5,0.5))
points(col(mat), nrow(mat) - row(mat) + 1,
pch=19,cex=mat*3,col=rainbow(ncol(mat),alpha=0.33)[col(mat)])
text(0.5, nrow(mat):1, vocab[index], adj=c(1,0.5),cex=0.7)
text(1:ncol(mat), -0.75, topicNames, adj=c(0.5,0),cex=0.7,srt=60)
par(mfrow=c(1,3))
par(mar=c(0,0,0,0))
for (index in list(1:60,61:120,121:179)) {
mat = topics[index,]
mat = mat / max(mat)
plot(0,0,col="white",ylim=c(-1,nrow(mat)),xlim=c(-5,ncol(mat)),axes=FALSE)
for(i in 1:nrow(mat)) lines(x=c(1,ncol(mat)),y=c(i,i), col=grey(0.5,0.5))
for(i in 1:ncol(mat)) lines(x=c(i,i),y=c(1,nrow(mat)), col=grey(0.5,0.5))
points(col(mat), nrow(mat) - row(mat) + 1,
pch=19,cex=mat*3,col=rainbow(ncol(mat),alpha=0.33)[col(mat)])
text(0.5, nrow(mat):1, shortNames[index], adj=c(1,0.5),cex=0.7)
text(1:ncol(mat), 0.5, topicNames, adj=c(1,0),cex=0.7,srt=60)
}
#dev.off()
pc = prcomp(topics,scale.=TRUE)
topicsScale = scale(topics,center=pc$center,scale=pc$scale)
wordScale = scale(diag(ncol(topics)),center=pc$center,scale=pc$scale)
topicPC = topicsScale %*% pc$rotation
wordPC = wordScale %*% pc$rotation
